---
title: "A Short Guide to Principal Component Analysis"
author: "Frederik Ziebell"
subtitle: '... and it''s Relation to Singular Value Decomposition'
output:
  html_document
---

<style type="text/css">
h1.title {
  text-align: center;
}
h3.subtitle {
  text-align: center;
}
h4.author {
  text-align: center;
}
</style>

\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator{\tr}{tr}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Theory
### Data setup
Let $X\in\mathbb{R}^{n\times p}$ be a column-centered data matrix with $n$ samples in the rows and $p$ features in the columns. The covariance matrix of the features is given by \[C=\frac{X^\top X}{n-1} \in\mathbb{R}^{p\times p},\] and satisfies \[C_{ij} = \Cov(x_i,x_j),\] where $x_i$ is the $i$-th column of X. It is important that $X$ is column-centered. Otherwise, $C$ is *not* the covariance matrix of the features.

### Principal axes and components
Since $C$ is a covariance matrix, it is positive semi-definite and can thus be diagonalized as \[C=VLV^\top\] where $L$ is a diagonal matrix of non-negative eigenvalues $\lambda_1,\ldots,\lambda_n$ and $V$ is an orthonormal matrix, i.e. $V^\top V=\mathbb{1}_p$, of eigenvectors, i.e. the $i$-th column $v_i$ of $V$ satisfies $Cv_i=\lambda_i v_i$. The vectors $v_1,\ldots,v_n$ are called the *principal axes* or *principal directions* of the data. Projections of the data onto the principal axes are called *principal components*. Since the columns of $V$ have unit norm, the entry $(XV)_{ij}$ is the projection of the $i$-th sample (row) of $X$ onto the $j$-th principal axis (column) of $V$. Thus, the $i$-th row of $XV$ is the projection of the $i$-th sample on the principal axes and hence the coordinates of this sample in the new coordinate system given by the principal axes. The $j$-th column of $XV$ is the $j$-th principal component.

### Singular value decomposition
As for any matrix, there exits a *singular value decomposition* (SVD) \[X=UDV^\top\] where $U$ is a $n\times n$ orthonormal matrix, $D$ a $n\times p$ matrix with only non-zero values on the main diagonal, and $V$ is a $p\times p$ orthonormal matrix. The entries on the main diagonal of $D$ are called singular values and are the square-roots of the eigenvalues of $X^\top X$ and $XX^\top$. The columns of $U$ (of $V$) are the left-singular (right-singular) vectors of $X$, i.e. the eigenvectors of $XX^\top$ (of $X^\top X$). 

It follows from the definition of the SVD that \[C=V\frac{D^2}{n-1}V^\top,\] which implies that the right-singular vectors $V$ are the principal axes, the eigenvalues $\lambda_i$ and singular values $d_i$ are related via $\lambda_i=d_i^2/(n-1)$ and the principal components are given by $XV=UD$. One can achieve a lower rank (rank $k$) approximation of $X$ by multiplying the first $k$ principal components $(UD)_k$ with the first $k$ principal axes $V_k^\top$.

If $n>p$, the last $n-p$ rows of $D$ are zero and thus the last $n-p$ columns of $U$ can be arbitrary and do not contribute to the factorization. In that case, a reduced SVD can be achived where $D$ is a $p\times p$ and $U$ a $n\times p$ matrix. Likewise, if $n<p$, a reduced SVD has $D$ as $n\times n$ matrix and $V$ as $p\times n$ matrix. 

### Variance decomposition
Since $X$ is column-centered, the sum of variances of all features is given by 
\[
\mathbb{V} =\frac{1}{n-1}\sum_{j=1}^p\sum_{i=1}^n(X_{ij})^2
=\tr(C)
=\sum_{j=1}^p \frac{d_j^2}{n-1}
=\sum_{j=1}^p \lambda_j.
\]

The second equality holds because the trace of a square matrix is the sum of its diagonal entries. The third equality results from the trace being the sum of the eigenvalues and $\{d_1,\ldots,d_p\}$ being the eigenvalues of $X^\top X$. This shows that the variance of the data $X$ is decomposed by its singular values contained in $D$, and by the eigenvalues of $X^\top X$ contained in $L$. 

The variance attributed to each principal component can be further decomposed onto the features: A rank $1$ approximation of $X$ from the first principal axis and component is given by the matrix $\tilde X$ with \[\tilde X_{ij} = (UD)_{i1}V_{ji}=d_1U_{i1}V_{j1}.\] The variance of feature $j$ satisfies \[\Var\left(\tilde X_{.j}\right) = \frac{d_1^2}{n-1}(V_{j1})^2.\] Together with the fact that $\sum_{j=1}^p (V_{j1})^2$, it implies that the squared entries of the principal axis vector contain the fraction of variance that is explained by the respective feature.

## Practical example

### The data
We analyze the `iris` dataset with principal component analysis (PCA) and SVD using *R*'s `prcomp` and `svd` functions.
```{r}
# load data
data <- as.matrix(iris[,1:4])

# setup variables as in theory section
X <- data
n <- nrow(data)
rownames(X) <- paste0("sample", 1:n)

# center the features
X <- scale(X, center = T, scale = F)

# do PCA and SVD
pca <- prcomp(X)
sv <- svd(X)

head(X)
```

### Results of `prcomp`
```{r}
# principal axes
head(pca$rotation)

# projection of X onto principal axes
head(pca$x)
```

### Compare `prcomp` and `svd`
```{r, results="hold"}
# don't compare object attributes
all_equal <- function(...) all.equal(..., check.attributes = FALSE)

# principal axes (principal directions) 
all_equal(pca$rotation, sv$v)

# projections of X onto principal axes
all_equal(pca$x, X %*% pca$rotation)
all_equal(pca$x , sv$u %*% diag(sv$d))

# full data reconstruction
all_equal(X, pca$x %*% t(pca$rotation))

# variance decomposition
all_equal(pca$sdev^2, ((sv$d)^2 /(n-1)))
all_equal(sum(pca$sdev^2), sum(X^2)/(n-1))

# eigenvalues
lambda <- eigen((t(X) %*% X)/(n-1))$values
all_equal(pca$sdev^2, lambda[lambda>0])
```

### Low-rank approximation from principal components
```{r}
# number of principal components to use
n_comp = 3
# reconstruction of column-centered X
X_approx <-  pca$x[,1:n_comp] %*% t(pca$rotation[,1:n_comp])

# add original column means to approximate original data
data_approx <-  scale(X_approx, center = -colMeans(data), scale = FALSE)

# check approximation error
all_equal(data_approx, data)
```

### Loadings decompose variance by feature
```{r, results="hold"}
# 1D approximation of the data
projections <- pca$x
loadings <- pca$rotation
X1 <- projections[,1] %*% t(loadings[,1])

# feature-wise variances
vars <- matrixStats::colVars(X1, useNames = T)

# relative proportions of feature-wise variances 
# are the squares of the loadings
all_equal(sum(vars), (pca$sdev^2)[1])
all_equal(vars / sum(vars), loadings[,1]^2)
```

## References
[1] https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca